{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31c72a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1ed7f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q decorator==5.0.9\n",
    "!pip install -q ipywidgets\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import re\n",
    "import imp\n",
    "import json\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import difflib as dl\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ede3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q cltk==1.0.22\n",
    "%pip install -q strsim\n",
    "%pip install -q leven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f812278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'features.model_features' from '../src/features/model_features.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing local modules\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import text_cleanup.text_cleanup as thesisCleanUp\n",
    "import preprocessing.text_preprocessing as thesisTextPreprocessing\n",
    "import data.reader as thesisDataReader\n",
    "import utils.utils as thesisUtils\n",
    "import features.tf_idf.n_gram as thesisTfIdfNgramFeatures\n",
    "import features.count_vectorizer.n_gram as thesisCountVectorizerNgramFeatures\n",
    "import similarities.cosine as thesisCosineSimilarities\n",
    "import similarities.levenshtein as thesisLevenshteinSimilarities\n",
    "import vocabulary.vocabulary as thesisVocabulary\n",
    "import features.lexical as thesisLexicalFeatures\n",
    "import similarities.cosine as thesisCosineSimilarity\n",
    "import text_cleanup.text_cleanup as thesisTextCleanUp\n",
    "import p_aligment.p_aligment as thesisPAligment\n",
    "import features.model_features as thesisModelFeatures\n",
    "\n",
    "imp.reload(thesisTfIdfNgramFeatures)\n",
    "imp.reload(thesisLexicalFeatures)\n",
    "imp.reload(thesisCosineSimilarity)\n",
    "imp.reload(thesisCleanUp)\n",
    "imp.reload(thesisTextPreprocessing)\n",
    "imp.reload(thesisDataReader)\n",
    "imp.reload(thesisUtils)\n",
    "\n",
    "imp.reload(thesisVocabulary)\n",
    "imp.reload(thesisCosineSimilarities)\n",
    "imp.reload(thesisTextCleanUp)\n",
    "imp.reload(thesisCountVectorizerNgramFeatures)\n",
    "imp.reload(thesisPAligment)\n",
    "imp.reload(thesisLevenshteinSimilarities)\n",
    "imp.reload(thesisModelFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aee417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_poorly_similar_with_chops_corpus_without_word_processing = thesisVocabulary.create_pre_proceed_corpus_from_processed_corpus(\n",
    "    thesisDataReader.get_london_poorly_similar_with_chops_corpus(),\n",
    "    thesisVocabulary.create_london_pre_post_processing_map()\n",
    ")\n",
    "zwickau_poorly_similar_with_chops_corpus_without_word_processing = thesisVocabulary.create_pre_proceed_corpus_from_processed_corpus(\n",
    "    thesisDataReader.get_zwickau_poorly_similar_with_chops_corpus(),\n",
    "    thesisVocabulary.create_zwickau_pre_post_processing_map()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0ae6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "burchard_candidate_version_based_london_without_word_processing = thesisVocabulary.create_pre_proceed_corpus_from_processed_corpus(\n",
    "    thesisDataReader.get_burchard_candidate_version_based_on_strongly_similar_london_base(),\n",
    "    thesisVocabulary.create_london_pre_post_processing_map()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ba8d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "london_poorly_similar_with_chops_corpus_without_word_processing_long_p = list(filter(lambda x: len(x.split()) > 20, london_poorly_similar_with_chops_corpus_without_word_processing))\n",
    "len(london_poorly_similar_with_chops_corpus_without_word_processing_long_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e539db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zwickau_poorly_similar_with_chops_corpus_without_word_processing_long_p = list(filter(lambda x: len(x.split()) > 20, zwickau_poorly_similar_with_chops_corpus_without_word_processing))\n",
    "len(zwickau_poorly_similar_with_chops_corpus_without_word_processing_long_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "556633ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(burchard_candidate_version_based_london_without_word_processing))\n",
    "burchard_candidate_version_based_london_without_word_processing_long_p = list(filter(lambda x: len(x.split()) > 20, burchard_candidate_version_based_london_without_word_processing))\n",
    "len(burchard_candidate_version_based_london_without_word_processing_long_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b657d",
   "metadata": {},
   "source": [
    "# burchard candidate vs london"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951a4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram_feature_name: 2_5_gram\n",
      "n_gram_feature_name: 2_5_gram\n"
     ]
    }
   ],
   "source": [
    "burchard_london_features_tfidf_2_5_gram_cosine_similarity_long_p_df = thesisModelFeatures.create_features_df(\n",
    "    london_poorly_similar_with_chops_corpus_without_word_processing_long_p,\n",
    "    None,\n",
    "    burchard_candidate_version_based_london_without_word_processing_long_p,\n",
    "    n_gram = (2,5),\n",
    "    features = { 'tfidf', 'inner_mean_cosine_similarity_score' }\n",
    "#     burchard_version_with_original_london_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78f6382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: SVM_linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: SVM_RBF\n",
      "running: DecisionTreeClassifier\n",
      "running: GaussianProcessClassifier\n",
      "running: RandomForestClassifier\n",
      "running: MLPClassifier\n",
      "running: GaussianNB\n",
      "running: KNeighborsClassifier\n",
      "running: AdaBoostClassifier\n",
      "running: QuadraticDiscriminantAnalysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "burchard_london_tf_idf_2_5_cosine_results_long_p = thesisModelFeatures.run_models(burchard_london_features_tfidf_2_5_gram_cosine_similarity_long_p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abc9af7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.766209</td>\n",
       "      <td>0.719048</td>\n",
       "      <td>0.717269</td>\n",
       "      <td>0.746021</td>\n",
       "      <td>0.730259</td>\n",
       "      <td>0.746021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianProcessClassifier</th>\n",
       "      <td>0.739933</td>\n",
       "      <td>0.724673</td>\n",
       "      <td>0.724190</td>\n",
       "      <td>0.740465</td>\n",
       "      <td>0.733636</td>\n",
       "      <td>0.740465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.738877</td>\n",
       "      <td>0.725476</td>\n",
       "      <td>0.723587</td>\n",
       "      <td>0.737838</td>\n",
       "      <td>0.732074</td>\n",
       "      <td>0.737838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.726435</td>\n",
       "      <td>0.714702</td>\n",
       "      <td>0.713771</td>\n",
       "      <td>0.723799</td>\n",
       "      <td>0.720573</td>\n",
       "      <td>0.723799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_RBF</th>\n",
       "      <td>0.810929</td>\n",
       "      <td>0.643988</td>\n",
       "      <td>0.613237</td>\n",
       "      <td>0.693844</td>\n",
       "      <td>0.638779</td>\n",
       "      <td>0.693844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.663752</td>\n",
       "      <td>0.661012</td>\n",
       "      <td>0.659873</td>\n",
       "      <td>0.666967</td>\n",
       "      <td>0.666367</td>\n",
       "      <td>0.666967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.615215</td>\n",
       "      <td>0.580476</td>\n",
       "      <td>0.559317</td>\n",
       "      <td>0.614565</td>\n",
       "      <td>0.580480</td>\n",
       "      <td>0.614565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.564421</td>\n",
       "      <td>0.548601</td>\n",
       "      <td>0.525635</td>\n",
       "      <td>0.584835</td>\n",
       "      <td>0.548405</td>\n",
       "      <td>0.584835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_linear</th>\n",
       "      <td>0.285548</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.363482</td>\n",
       "      <td>0.571096</td>\n",
       "      <td>0.415228</td>\n",
       "      <td>0.571096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.560186</td>\n",
       "      <td>0.560446</td>\n",
       "      <td>0.549287</td>\n",
       "      <td>0.552027</td>\n",
       "      <td>0.550432</td>\n",
       "      <td>0.552027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               precision_macro  recall_macro  f1_macro  \\\n",
       "RandomForestClassifier                0.766209      0.719048  0.717269   \n",
       "GaussianProcessClassifier             0.739933      0.724673  0.724190   \n",
       "MLPClassifier                         0.738877      0.725476  0.723587   \n",
       "AdaBoostClassifier                    0.726435      0.714702  0.713771   \n",
       "SVM_RBF                               0.810929      0.643988  0.613237   \n",
       "DecisionTreeClassifier                0.663752      0.661012  0.659873   \n",
       "KNeighborsClassifier                  0.615215      0.580476  0.559317   \n",
       "GaussianNB                            0.564421      0.548601  0.525635   \n",
       "SVM_linear                            0.285548      0.500000  0.363482   \n",
       "QuadraticDiscriminantAnalysis         0.560186      0.560446  0.549287   \n",
       "\n",
       "                               f1_micro  f1_weighted  accuracy  \n",
       "RandomForestClassifier         0.746021     0.730259  0.746021  \n",
       "GaussianProcessClassifier      0.740465     0.733636  0.740465  \n",
       "MLPClassifier                  0.737838     0.732074  0.737838  \n",
       "AdaBoostClassifier             0.723799     0.720573  0.723799  \n",
       "SVM_RBF                        0.693844     0.638779  0.693844  \n",
       "DecisionTreeClassifier         0.666967     0.666367  0.666967  \n",
       "KNeighborsClassifier           0.614565     0.580480  0.614565  \n",
       "GaussianNB                     0.584835     0.548405  0.584835  \n",
       "SVM_linear                     0.571096     0.415228  0.571096  \n",
       "QuadraticDiscriminantAnalysis  0.552027     0.550432  0.552027  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burchard_london_tf_idf_2_5_cosine_results_long_p[0].sort_values(by=['accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99255bed",
   "metadata": {},
   "source": [
    "## burchard candidate vs london greed search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ff530f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing classifiers: ['SVC']\n",
      "running: SVC\n",
      "0.7541291291291292\n",
      "testing classifiers: ['DecisionTreeClassifier']\n",
      "running: DecisionTreeClassifier\n",
      "0.7021771771771773\n",
      "testing classifiers: ['GaussianProcessClassifier']\n",
      "running: GaussianProcessClassifier\n",
      "0.7484234234234235\n",
      "testing classifiers: ['RandomForestClassifier']\n",
      "running: RandomForestClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.74324324 0.75165165 0.75705706 0.75157658 0.74602102 0.7515015\n",
      " 0.7542042  0.75427928 0.74324324 0.75165165 0.75705706 0.75157658\n",
      " 0.74602102 0.7515015  0.7542042  0.75427928 0.58483483 0.5710961\n",
      " 0.5710961  0.5710961  0.5710961  0.5710961  0.5710961  0.5710961\n",
      " 0.74864865 0.74346847 0.75165165 0.74894895 0.74887387 0.75427928\n",
      " 0.75968468 0.76516517 0.74864865 0.74346847 0.75165165 0.74894895\n",
      " 0.74887387 0.75427928 0.75968468 0.76516517 0.57672673 0.57672673\n",
      " 0.57387387 0.5710961  0.5710961  0.5710961  0.5710961  0.5710961\n",
      " 0.75112613 0.74872372 0.76524024 0.76516517 0.76246246 0.76246246\n",
      " 0.75705706 0.75975976 0.75112613 0.74872372 0.76524024 0.76516517\n",
      " 0.76246246 0.76246246 0.75705706 0.75975976 0.60397898 0.59294294\n",
      " 0.5710961  0.5737988  0.5710961  0.5710961  0.5737988  0.5710961\n",
      " 0.74602102 0.76524024 0.75975976 0.75435435 0.76516517 0.77875375\n",
      " 0.77334835 0.76524024 0.74602102 0.76524024 0.75975976 0.75435435\n",
      " 0.76516517 0.77875375 0.77334835 0.76524024 0.60382883 0.60660661\n",
      " 0.58460961 0.58738739 0.5792042  0.57657658 0.5737988  0.5737988\n",
      " 0.74339339 0.76501502 0.77327327 0.76786787 0.76516517 0.75975976\n",
      " 0.75975976 0.76524024 0.74339339 0.76501502 0.77327327 0.76786787\n",
      " 0.76516517 0.75975976 0.75975976 0.76524024 0.60668168 0.61216216\n",
      " 0.60105105 0.59827327 0.59827327 0.5981982  0.59279279 0.59001502\n",
      " 0.76779279 0.76516517 0.77882883 0.78153153 0.77064565 0.77342342\n",
      " 0.76253754 0.76794294 0.76779279 0.76516517 0.77882883 0.78153153\n",
      " 0.77064565 0.77342342 0.76253754 0.76794294 0.61741742 0.62567568\n",
      " 0.60105105 0.5954955  0.60097598 0.5981982  0.58738739 0.59009009\n",
      " 0.75405405 0.75705706 0.75705706 0.75172673 0.75713213 0.76516517\n",
      " 0.75705706 0.75705706 0.75405405 0.75705706 0.75705706 0.75172673\n",
      " 0.75713213 0.76516517 0.75705706 0.75705706 0.62852853 0.63393393\n",
      " 0.62845345 0.61456456 0.61734234 0.60367868 0.5954955  0.5954955\n",
      " 0.75975976 0.77605105 0.77057057 0.76493994 0.7704955  0.76231231\n",
      " 0.75968468 0.76516517 0.75975976 0.77605105 0.77057057 0.76493994\n",
      " 0.7704955  0.76231231 0.75968468 0.76516517 0.60923423 0.63678679\n",
      " 0.63415916 0.62304805 0.62297297 0.61749249 0.60375375 0.61186186\n",
      " 0.76801802 0.76501502 0.77034535 0.75953453 0.7731982  0.76779279\n",
      " 0.77327327 0.77334835 0.76801802 0.76501502 0.77034535 0.75953453\n",
      " 0.7731982  0.76779279 0.77327327 0.77334835 0.61201201 0.64256757\n",
      " 0.64211712 0.62837838 0.63918919 0.62297297 0.62004505 0.62282282\n",
      " 0.74331832 0.74887387 0.74872372 0.74339339 0.74339339 0.74331832\n",
      " 0.7515015  0.7542042  0.74331832 0.74887387 0.74872372 0.74339339\n",
      " 0.74339339 0.74331832 0.7515015  0.7542042  0.57117117 0.5710961\n",
      " 0.5710961  0.5710961  0.5710961  0.5710961  0.5710961  0.5710961\n",
      " 0.73228228 0.76794294 0.76779279 0.76516517 0.75427928 0.76238739\n",
      " 0.75975976 0.76524024 0.73228228 0.76794294 0.76779279 0.76516517\n",
      " 0.75427928 0.76238739 0.75975976 0.76524024 0.5737988  0.57927928\n",
      " 0.5710961  0.5710961  0.5710961  0.5710961  0.5710961  0.5710961\n",
      " 0.74046547 0.76238739 0.76238739 0.75427928 0.75705706 0.76524024\n",
      " 0.76261261 0.75983483 0.74046547 0.76238739 0.76238739 0.75427928\n",
      " 0.75705706 0.76524024 0.76261261 0.75983483 0.58190691 0.59024024\n",
      " 0.5792042  0.5765015  0.5710961  0.5710961  0.5737988  0.5737988\n",
      " 0.74339339 0.75157658 0.77597598 0.76231231 0.75960961 0.77327327\n",
      " 0.75983483 0.7707958  0.74339339 0.75157658 0.77597598 0.76231231\n",
      " 0.75960961 0.77327327 0.75983483 0.7707958  0.58483483 0.59572072\n",
      " 0.58190691 0.58190691 0.58190691 0.58190691 0.5737988  0.5737988\n",
      " 0.74594595 0.75705706 0.75690691 0.76238739 0.76253754 0.78145646\n",
      " 0.75172673 0.74902402 0.74594595 0.75705706 0.75690691 0.76238739\n",
      " 0.76253754 0.78145646 0.75172673 0.74902402 0.58761261 0.58768769\n",
      " 0.59024024 0.58198198 0.59009009 0.58468468 0.5737988  0.5765015\n",
      " 0.72387387 0.76223724 0.75960961 0.75705706 0.76794294 0.76794294\n",
      " 0.76516517 0.76524024 0.72387387 0.76223724 0.75960961 0.75705706\n",
      " 0.76794294 0.76794294 0.76516517 0.76524024 0.59864865 0.60960961\n",
      " 0.60375375 0.5954955  0.5954955  0.59279279 0.58738739 0.59009009\n",
      " 0.74324324 0.7704955  0.77327327 0.77597598 0.77327327 0.77064565\n",
      " 0.77072072 0.76253754 0.74324324 0.7704955  0.77327327 0.77597598\n",
      " 0.77327327 0.77064565 0.77072072 0.76253754 0.61741742 0.62567568\n",
      " 0.62282282 0.60915916 0.61186186 0.59557057 0.5954955  0.5954955\n",
      " 0.74316817 0.76779279 0.77057057 0.75960961 0.76238739 0.76786787\n",
      " 0.75705706 0.76253754 0.74316817 0.76779279 0.77057057 0.75960961\n",
      " 0.76238739 0.76786787 0.75705706 0.76253754 0.59279279 0.62575075\n",
      " 0.62282282 0.61734234 0.62274775 0.61463964 0.60645646 0.60915916\n",
      " 0.73475976 0.77582583 0.76771772 0.77042042 0.76779279 0.7759009\n",
      " 0.77605105 0.77057057 0.73475976 0.77582583 0.76771772 0.77042042\n",
      " 0.76779279 0.7759009  0.77605105 0.77057057 0.61726727 0.63663664\n",
      " 0.62807808 0.61734234 0.61463964 0.61186186 0.62004505 0.61456456\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the train scores are non-finite: [0.92682325 0.93623929 0.93715207 0.93776089 0.9426149  0.9453477\n",
      " 0.9453477  0.9447398  0.92682325 0.93623929 0.93715207 0.93776089\n",
      " 0.9426149  0.9453477  0.9453477  0.9447398  0.65360505 0.64146081\n",
      " 0.63023395 0.61354149 0.60686562 0.59653956 0.5925928  0.59138252\n",
      " 0.97175831 0.97783366 0.98086949 0.98572626 0.98724325 0.98663719\n",
      " 0.98815787 0.9884609  0.97175831 0.97783366 0.98086949 0.98572626\n",
      " 0.98724325 0.98663719 0.98815787 0.9884609  0.72495349 0.72706733\n",
      " 0.71706272 0.69610298 0.69337939 0.68063461 0.67030948 0.66424611\n",
      " 0.98573363 0.99362255 0.9960514  0.99787234 0.9993921  0.9993921\n",
      " 0.99969605 1.         0.98573363 0.99362255 0.9960514  0.99787234\n",
      " 0.9993921  0.9993921  0.99969605 1.         0.79142765 0.81298793\n",
      " 0.79962605 0.79324123 0.79841485 0.79385926 0.78627061 0.77808234\n",
      " 0.99514046 0.99908907 1.         1.         1.         1.\n",
      " 1.         1.         0.99514046 0.99908907 1.         1.\n",
      " 1.         1.         1.         1.         0.85549599 0.88645206\n",
      " 0.88373123 0.88221148 0.88372386 0.88554205 0.88554757 0.88099751\n",
      " 0.99848301 0.99939302 1.         1.         1.         1.\n",
      " 1.         1.         0.99848301 0.99939302 1.         1.\n",
      " 1.         1.         1.         1.         0.89982223 0.93716128\n",
      " 0.93473611 0.9371622  0.94141199 0.94535599 0.94991158 0.94687759\n",
      " 0.99939302 1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99939302 1.         1.         1.\n",
      " 1.         1.         1.         1.         0.92896657 0.96417611\n",
      " 0.96418163 0.96630008 0.97146449 0.9738915  0.97692825 0.97480427\n",
      " 0.99939302 1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99939302 1.         1.         1.\n",
      " 1.         1.         1.         1.         0.95870959 0.98178318\n",
      " 0.98421019 0.98238648 0.98572718 0.98633508 0.98785392 0.98633508\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.97085106 0.98694575\n",
      " 0.99241227 0.99149765 0.99301557 0.9918016  0.99362255 0.99271161\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.98087041 0.99301925\n",
      " 0.99392926 0.99453532 0.99544533 0.99483743 0.99514046 0.99362347\n",
      " 0.91619969 0.92196832 0.9186359  0.91985447 0.92409966 0.92713825\n",
      " 0.9280455  0.92744128 0.91619969 0.92196832 0.9186359  0.91985447\n",
      " 0.92409966 0.92713825 0.9280455  0.92744128 0.6390338  0.62536889\n",
      " 0.61839827 0.59927328 0.59411348 0.58500138 0.57893064 0.58014829\n",
      " 0.95809984 0.96750852 0.96994197 0.97327991 0.9757106  0.97753155\n",
      " 0.9775297  0.9763139  0.95809984 0.96750852 0.96994197 0.97327991\n",
      " 0.9757106  0.97753155 0.9775297  0.9763139  0.70461177 0.70400018\n",
      " 0.68882564 0.66392926 0.66150502 0.65179147 0.64237727 0.64147094\n",
      " 0.98026895 0.9872497  0.98937368 0.99301833 0.99453532 0.99605232\n",
      " 0.99605324 0.9972672  0.98026895 0.9872497  0.98937368 0.99301833\n",
      " 0.99453532 0.99605232 0.99605324 0.9972672  0.7671493  0.78809063\n",
      " 0.76685364 0.75895275 0.76169107 0.75774523 0.74925394 0.74287741\n",
      " 0.99089067 0.99878604 0.99969605 0.99969605 0.9993921  1.\n",
      " 1.         1.         0.99089067 0.99878604 0.99969605 0.99969605\n",
      " 0.9993921  1.         1.         1.         0.83059409 0.86035\n",
      " 0.8527724  0.84669614 0.85551257 0.85581008 0.85338123 0.8470047\n",
      " 0.99787602 0.99969605 0.99969697 1.         1.         1.\n",
      " 1.         1.         0.99787602 0.99969605 0.99969697 1.\n",
      " 1.         1.         1.         1.         0.87827392 0.91530994\n",
      " 0.91379018 0.91408769 0.92168094 0.92320162 0.92806024 0.92806208\n",
      " 0.9978751  1.         1.         1.         1.         1.\n",
      " 1.         1.         0.9978751  1.         1.         1.\n",
      " 1.         1.         1.         1.         0.91317952 0.94990513\n",
      " 0.94869761 0.95051395 0.96023027 0.95993184 0.96357189 0.962357\n",
      " 0.99908907 1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99908907 1.         1.         1.\n",
      " 1.         1.         1.         1.         0.94505112 0.96872709\n",
      " 0.97207055 0.97419361 0.97996408 0.98269411 0.9845169  0.98239016\n",
      " 0.99939302 1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99939302 1.         1.         1.\n",
      " 1.         1.         1.         1.         0.96083817 0.98208713\n",
      " 0.9845169  0.98725154 0.98755549 0.9893746  0.99089159 0.99119554\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.9772322  0.98785852\n",
      " 0.99271346 0.99241043 0.99362531 0.99392834 0.99483927 0.99483743\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7815315315315315\n",
      "testing classifiers: ['GaussianNB']\n",
      "running: GaussianNB\n",
      "0.6283033033033034\n",
      "testing classifiers: ['KNeighborsClassifier']\n",
      "running: KNeighborsClassifier\n",
      "0.6145645645645647\n",
      "testing classifiers: ['AdaBoostClassifier']\n",
      "running: AdaBoostClassifier\n",
      "0.7595345345345346\n"
     ]
    }
   ],
   "source": [
    "resp = []\n",
    "for cls in ['SVC', 'DecisionTreeClassifier', 'GaussianProcessClassifier', 'RandomForestClassifier', 'GaussianNB', 'KNeighborsClassifier', 'AdaBoostClassifier']:\n",
    "    grid_search_cv_result = thesisModelFeatures.run_grid_search_cv(burchard_london_features_tfidf_2_5_gram_cosine_similarity_long_p_df, [cls])\n",
    "    resp.append([cls, grid_search_cv_result[1][0].best_score_])\n",
    "    print(grid_search_cv_result[1][0].best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09c5a038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['SVC', 0.7541291291291292],\n",
       " ['DecisionTreeClassifier', 0.7021771771771773],\n",
       " ['GaussianProcessClassifier', 0.7484234234234235],\n",
       " ['RandomForestClassifier', 0.7815315315315315],\n",
       " ['GaussianNB', 0.6283033033033034],\n",
       " ['KNeighborsClassifier', 0.6145645645645647],\n",
       " ['AdaBoostClassifier', 0.7595345345345346]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa7e80",
   "metadata": {},
   "source": [
    "## random london VS random london"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b113ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indexes = np.split(\n",
    "    np.random.choice(\n",
    "        range(\n",
    "            0, \n",
    "            len(london_poorly_similar_with_chops_corpus_without_word_processing_long_p)\n",
    "        ), \n",
    "        size=len(london_poorly_similar_with_chops_corpus_without_word_processing_long_p), \n",
    "        replace=False\n",
    "    ),\n",
    "    1\n",
    ")\n",
    "random_indexes = [random_indexes[0][:78], random_indexes[0][79:]]\n",
    "random_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f957b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_random_chunk_1 = [ london_poorly_similar_with_chops_corpus_without_word_processing_long_p[i] for i in random_indexes[0] ]\n",
    "london_random_chunk_2 = [ london_poorly_similar_with_chops_corpus_without_word_processing_long_p[i] for i in random_indexes[1] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96546afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_london_features_tfidf_2_5_gram_cosine_similarity_long_p_df = thesisModelFeatures.create_features_df(\n",
    "    london_random_chunk_1,\n",
    "    None,\n",
    "    london_random_chunk_2,\n",
    "    n_gram = (2,5),\n",
    "    features = { 'tfidf', 'inner_mean_cosine_similarity_score' }\n",
    "#     burchard_version_with_original_london_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_london_features_tfidf_2_5_gram_cosine_similarity_long_p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfed210",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_london_tf_idf_2_5_cosine_results_long_p = thesisModelFeatures.run_models(london_london_features_tfidf_2_5_gram_cosine_similarity_long_p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_london_tf_idf_2_5_cosine_results_long_p.sort_values(by=['accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60c83e",
   "metadata": {},
   "source": [
    "# burchard candidate vs zwickau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9669103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram_feature_name: 2_5_gram\n",
      "n_gram_feature_name: 2_5_gram\n"
     ]
    }
   ],
   "source": [
    "burchard_zwickau_features_tfidf_2_5_gram_cosine_similarity_long_p_df = thesisModelFeatures.create_features_df(\n",
    "    None,\n",
    "    zwickau_poorly_similar_with_chops_corpus_without_word_processing_long_p,\n",
    "    burchard_candidate_version_based_london_without_word_processing_long_p,\n",
    "    n_gram = (2,5),\n",
    "    features = { 'tfidf', 'inner_mean_cosine_similarity_score' }\n",
    "#     burchard_version_with_original_london_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a889e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>corpus_version_label</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a a</th>\n",
       "      <th>a ac</th>\n",
       "      <th>a ad</th>\n",
       "      <th>a al</th>\n",
       "      <th>a ap</th>\n",
       "      <th>a b</th>\n",
       "      <th>...</th>\n",
       "      <th>zra h</th>\n",
       "      <th>zrae</th>\n",
       "      <th>zrael</th>\n",
       "      <th>zrah</th>\n",
       "      <th>zrahe</th>\n",
       "      <th>zu</th>\n",
       "      <th>zur</th>\n",
       "      <th>zuri</th>\n",
       "      <th>zurio</th>\n",
       "      <th>inner_mean_cosine_similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.038287</td>\n",
       "      <td>0.018428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.169794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.053370</td>\n",
       "      <td>0.014449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.032915</td>\n",
       "      <td>0.023764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>204.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.045568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>205.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.043055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>206.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.044101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>207.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.032233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.198659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>208.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.045106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 49713 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  corpus_version_label         a        a    a a   a ac   a ad  \\\n",
       "0      0.0                   1.0  0.009905  0.000000   0.0    0.0    0.0   \n",
       "1      1.0                   1.0  0.038287  0.018428   0.0    0.0    0.0   \n",
       "2      2.0                   1.0  0.053370  0.014449   0.0    0.0    0.0   \n",
       "3      3.0                   1.0  0.033969  0.000000   0.0    0.0    0.0   \n",
       "4      4.0                   1.0  0.032915  0.023764   0.0    0.0    0.0   \n",
       "..     ...                   ...       ...       ...   ...    ...    ...   \n",
       "370  204.0                   2.0  0.045568  0.000000   0.0    0.0    0.0   \n",
       "371  205.0                   2.0  0.043055  0.000000   0.0    0.0    0.0   \n",
       "372  206.0                   2.0  0.044101  0.000000   0.0    0.0    0.0   \n",
       "373  207.0                   2.0  0.032233  0.000000   0.0    0.0    0.0   \n",
       "374  208.0                   2.0  0.045106  0.000000   0.0    0.0    0.0   \n",
       "\n",
       "      a al   a ap   a b  ...  zra h  zrae  zrael  zrah  zrahe   zu  zur  zuri  \\\n",
       "0      0.0    0.0   0.0  ...    0.0   0.0    0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "1      0.0    0.0   0.0  ...    0.0   0.0    0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "2      0.0    0.0   0.0  ...    0.0   0.0    0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "3      0.0    0.0   0.0  ...    0.0   0.0    0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "4      0.0    0.0   0.0  ...    0.0   0.0    0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "..     ...    ...   ...  ...    ...   ...    ...   ...    ...  ...  ...   ...   \n",
       "370    0.0    0.0   0.0  ...    0.0   0.0    0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "371    0.0    0.0   0.0  ...    0.0   0.0    0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "372    0.0    0.0   0.0  ...    0.0   0.0    0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "373    0.0    0.0   0.0  ...    0.0   0.0    0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "374    0.0    0.0   0.0  ...    0.0   0.0    0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "\n",
       "     zurio  inner_mean_cosine_similarity_score  \n",
       "0      0.0                            0.089736  \n",
       "1      0.0                            0.169794  \n",
       "2      0.0                            0.110581  \n",
       "3      0.0                            0.117864  \n",
       "4      0.0                            0.086517  \n",
       "..     ...                                 ...  \n",
       "370    0.0                            0.231712  \n",
       "371    0.0                            0.155838  \n",
       "372    0.0                            0.255245  \n",
       "373    0.0                            0.198659  \n",
       "374    0.0                            0.157189  \n",
       "\n",
       "[375 rows x 49713 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burchard_zwickau_features_tfidf_2_5_gram_cosine_similarity_long_p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10b12918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: SVM_linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: SVM_RBF\n",
      "running: DecisionTreeClassifier\n",
      "running: GaussianProcessClassifier\n",
      "running: RandomForestClassifier\n",
      "running: MLPClassifier\n",
      "running: GaussianNB\n",
      "running: KNeighborsClassifier\n",
      "running: AdaBoostClassifier\n",
      "running: QuadraticDiscriminantAnalysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "burchard_zwickau_tf_idf_2_5_cosine_results_long_p = thesisModelFeatures.run_models(burchard_zwickau_features_tfidf_2_5_gram_cosine_similarity_long_p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c423fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.853053</td>\n",
       "      <td>0.843722</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.848435</td>\n",
       "      <td>0.847453</td>\n",
       "      <td>0.848435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianProcessClassifier</th>\n",
       "      <td>0.755550</td>\n",
       "      <td>0.739653</td>\n",
       "      <td>0.738649</td>\n",
       "      <td>0.747084</td>\n",
       "      <td>0.743223</td>\n",
       "      <td>0.747084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.749698</td>\n",
       "      <td>0.735215</td>\n",
       "      <td>0.734695</td>\n",
       "      <td>0.744523</td>\n",
       "      <td>0.739920</td>\n",
       "      <td>0.744523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.751054</td>\n",
       "      <td>0.734891</td>\n",
       "      <td>0.733459</td>\n",
       "      <td>0.741821</td>\n",
       "      <td>0.737887</td>\n",
       "      <td>0.741821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.722642</td>\n",
       "      <td>0.720208</td>\n",
       "      <td>0.717356</td>\n",
       "      <td>0.722546</td>\n",
       "      <td>0.720964</td>\n",
       "      <td>0.722546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_RBF</th>\n",
       "      <td>0.719491</td>\n",
       "      <td>0.695952</td>\n",
       "      <td>0.690971</td>\n",
       "      <td>0.712304</td>\n",
       "      <td>0.699485</td>\n",
       "      <td>0.712304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.692289</td>\n",
       "      <td>0.663612</td>\n",
       "      <td>0.659397</td>\n",
       "      <td>0.677596</td>\n",
       "      <td>0.667183</td>\n",
       "      <td>0.677596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.666464</td>\n",
       "      <td>0.566215</td>\n",
       "      <td>0.522792</td>\n",
       "      <td>0.605690</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.605690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.677052</td>\n",
       "      <td>0.554013</td>\n",
       "      <td>0.489544</td>\n",
       "      <td>0.599929</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>0.599929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_linear</th>\n",
       "      <td>0.278698</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.357881</td>\n",
       "      <td>0.557397</td>\n",
       "      <td>0.399031</td>\n",
       "      <td>0.557397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               precision_macro  recall_macro  f1_macro  \\\n",
       "AdaBoostClassifier                    0.853053      0.843722  0.845014   \n",
       "GaussianProcessClassifier             0.755550      0.739653  0.738649   \n",
       "RandomForestClassifier                0.749698      0.735215  0.734695   \n",
       "MLPClassifier                         0.751054      0.734891  0.733459   \n",
       "DecisionTreeClassifier                0.722642      0.720208  0.717356   \n",
       "SVM_RBF                               0.719491      0.695952  0.690971   \n",
       "QuadraticDiscriminantAnalysis         0.692289      0.663612  0.659397   \n",
       "GaussianNB                            0.666464      0.566215  0.522792   \n",
       "KNeighborsClassifier                  0.677052      0.554013  0.489544   \n",
       "SVM_linear                            0.278698      0.500000  0.357881   \n",
       "\n",
       "                               f1_micro  f1_weighted  accuracy  \n",
       "AdaBoostClassifier             0.848435     0.847453  0.848435  \n",
       "GaussianProcessClassifier      0.747084     0.743223  0.747084  \n",
       "RandomForestClassifier         0.744523     0.739920  0.744523  \n",
       "MLPClassifier                  0.741821     0.737887  0.741821  \n",
       "DecisionTreeClassifier         0.722546     0.720964  0.722546  \n",
       "SVM_RBF                        0.712304     0.699485  0.712304  \n",
       "QuadraticDiscriminantAnalysis  0.677596     0.667183  0.677596  \n",
       "GaussianNB                     0.605690     0.545000  0.605690  \n",
       "KNeighborsClassifier           0.599929     0.517000  0.599929  \n",
       "SVM_linear                     0.557397     0.399031  0.557397  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burchard_zwickau_tf_idf_2_5_cosine_results_long_p[0].sort_values(by=['accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce14d3f",
   "metadata": {},
   "source": [
    "## burchard candidate vs zwickau greed search cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73561508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing classifiers: ['SVC']\n",
      "running: SVC\n",
      "0.768421052631579\n",
      "testing classifiers: ['DecisionTreeClassifier']\n",
      "running: DecisionTreeClassifier\n",
      "0.7571123755334283\n",
      "testing classifiers: ['GaussianProcessClassifier']\n",
      "running: GaussianProcessClassifier\n",
      "0.7524182076813657\n",
      "testing classifiers: ['RandomForestClassifier']\n",
      "running: RandomForestClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.71187767 0.72560455 0.7440256  0.74665718 0.73342817 0.73349929\n",
      " 0.73890469 0.73897582 0.71187767 0.72560455 0.7440256  0.74665718\n",
      " 0.73342817 0.73349929 0.73890469 0.73897582 0.56002845 0.56536273\n",
      " 0.56536273 0.56002845 0.56002845 0.55739687 0.56002845 0.55739687\n",
      " 0.72510669 0.72283073 0.73086771 0.73349929 0.74153627 0.74416785\n",
      " 0.73086771 0.73605974 0.72510669 0.72283073 0.73086771 0.73349929\n",
      " 0.74153627 0.74416785 0.73086771 0.73605974 0.57901849 0.57361309\n",
      " 0.57076814 0.57339972 0.56536273 0.56266003 0.56536273 0.56273115\n",
      " 0.71991465 0.73364154 0.72290185 0.73093883 0.73357041 0.73627312\n",
      " 0.73620199 0.73883357 0.71991465 0.73364154 0.72290185 0.73093883\n",
      " 0.73357041 0.73627312 0.73620199 0.73883357 0.57091038 0.58691323\n",
      " 0.6002845  0.6029872  0.58961593 0.58954481 0.59480797 0.59224751\n",
      " 0.71201991 0.7254623  0.73890469 0.74694168 0.73883357 0.74153627\n",
      " 0.74950213 0.74687055 0.71201991 0.7254623  0.73890469 0.74694168\n",
      " 0.73883357 0.74153627 0.74950213 0.74687055 0.59765292 0.63492176\n",
      " 0.61358464 0.6215505  0.59210526 0.61621622 0.61095306 0.60832148\n",
      " 0.73051209 0.7173542  0.74139403 0.73876245 0.74672831 0.7497155\n",
      " 0.74687055 0.74679943 0.73051209 0.7173542  0.74139403 0.73876245\n",
      " 0.74672831 0.7497155  0.74687055 0.74679943 0.62688478 0.65064011\n",
      " 0.66159317 0.65106686 0.65099573 0.65092461 0.65647226 0.64587482\n",
      " 0.72290185 0.71756757 0.7470128  0.72560455 0.72837838 0.73093883\n",
      " 0.75220484 0.74957326 0.72290185 0.71756757 0.7470128  0.72560455\n",
      " 0.72837838 0.73093883 0.75220484 0.74957326 0.64039829 0.65611664\n",
      " 0.65348506 0.64544808 0.65604552 0.66422475 0.66955903 0.66159317\n",
      " 0.71465149 0.74153627 0.75490754 0.75227596 0.75761024 0.74708393\n",
      " 0.7443101  0.74950213 0.71465149 0.74153627 0.75490754 0.75227596\n",
      " 0.75761024 0.74708393 0.7443101  0.74950213 0.66152205 0.65874822\n",
      " 0.66130868 0.65355619 0.65881935 0.66152205 0.67482219 0.66415363\n",
      " 0.73321479 0.73897582 0.75227596 0.74687055 0.7443101  0.74964438\n",
      " 0.75768137 0.74943101 0.73321479 0.73897582 0.75227596 0.74687055\n",
      " 0.7443101  0.74964438 0.75768137 0.74943101 0.62446657 0.6559744\n",
      " 0.67467994 0.65611664 0.67219061 0.67738265 0.685633   0.6802276\n",
      " 0.70945946 0.73349929 0.75746799 0.76827881 0.75746799 0.75497866\n",
      " 0.76031294 0.76024182 0.70945946 0.73349929 0.75746799 0.76827881\n",
      " 0.75746799 0.75497866 0.76031294 0.76024182 0.65113798 0.6616643\n",
      " 0.66692745 0.66941679 0.67489331 0.66948791 0.66692745 0.66429587\n",
      " 0.71458037 0.73876245 0.74395448 0.73847795 0.7254623  0.73349929\n",
      " 0.74153627 0.73890469 0.71458037 0.73876245 0.74395448 0.73847795\n",
      " 0.7254623  0.73349929 0.74153627 0.73890469 0.55199147 0.56266003\n",
      " 0.56806543 0.56273115 0.56273115 0.55739687 0.55739687 0.55739687\n",
      " 0.72809388 0.73335704 0.74957326 0.73890469 0.75227596 0.75476529\n",
      " 0.74943101 0.75739687 0.72809388 0.73335704 0.74957326 0.73890469\n",
      " 0.75227596 0.75476529 0.74943101 0.75739687 0.57866287 0.57880512\n",
      " 0.57887624 0.57617354 0.56806543 0.56536273 0.56273115 0.56273115\n",
      " 0.72283073 0.73876245 0.74409673 0.75497866 0.75753912 0.76009957\n",
      " 0.7628734  0.75476529 0.72283073 0.73876245 0.74409673 0.75497866\n",
      " 0.75753912 0.76009957 0.7628734  0.75476529 0.61621622 0.61621622\n",
      " 0.59231863 0.59480797 0.58684211 0.5841394  0.57866287 0.5841394\n",
      " 0.71721195 0.74416785 0.74950213 0.74687055 0.74935989 0.75206259\n",
      " 0.75469417 0.75739687 0.71721195 0.74416785 0.74950213 0.74687055\n",
      " 0.74935989 0.75206259 0.75469417 0.75739687 0.59217639 0.62681366\n",
      " 0.6083926  0.61081081 0.61635846 0.60547653 0.59480797 0.60284495\n",
      " 0.72532006 0.73335704 0.75184922 0.75483642 0.74672831 0.74679943\n",
      " 0.76280228 0.75483642 0.72532006 0.73335704 0.75184922 0.75483642\n",
      " 0.74672831 0.74679943 0.76280228 0.75483642 0.62446657 0.64829303\n",
      " 0.64302987 0.62709815 0.63499289 0.64032717 0.64288762 0.62972973\n",
      " 0.68805121 0.72809388 0.73349929 0.74423898 0.75490754 0.7628734\n",
      " 0.75234708 0.77083926 0.68805121 0.72809388 0.73349929 0.74423898\n",
      " 0.75490754 0.7628734  0.75234708 0.77083926 0.61386913 0.64573257\n",
      " 0.65633001 0.64274538 0.6428165  0.64295875 0.62702703 0.62987198\n",
      " 0.73058321 0.72837838 0.74950213 0.75220484 0.75206259 0.74935989\n",
      " 0.75746799 0.75746799 0.73058321 0.72837838 0.74950213 0.75220484\n",
      " 0.75206259 0.74935989 0.75746799 0.75746799 0.62681366 0.65640114\n",
      " 0.65355619 0.64295875 0.65640114 0.66692745 0.66699858 0.664367\n",
      " 0.69871977 0.71472262 0.73065434 0.73086771 0.73349929 0.73620199\n",
      " 0.73627312 0.74943101 0.69871977 0.71472262 0.73065434 0.73086771\n",
      " 0.73349929 0.73620199 0.73627312 0.74943101 0.63755334 0.664367\n",
      " 0.65647226 0.6616643  0.68300142 0.68826458 0.68819346 0.67766714\n",
      " 0.70931721 0.74423898 0.73357041 0.74153627 0.74423898 0.74950213\n",
      " 0.74146515 0.74687055 0.70931721 0.74423898 0.73357041 0.74153627\n",
      " 0.74423898 0.74950213 0.74146515 0.74687055 0.65128023 0.66714083\n",
      " 0.63229018 0.64829303 0.66152205 0.68278805 0.69054054 0.67211949\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "/Users/sasha.kruglyak/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the train scores are non-finite: [0.94874019 0.96118729 0.9674126  0.96977947 0.97214809 0.97274244\n",
      " 0.97185311 0.97511193 0.94874019 0.96118729 0.9674126  0.96977947\n",
      " 0.97214809 0.97274244 0.97185311 0.97511193 0.69569382 0.69894738\n",
      " 0.69568943 0.67525065 0.66812986 0.65480133 0.65036258 0.64680965\n",
      " 0.97511369 0.98874247 0.99140783 0.99140783 0.99466753 0.99466753\n",
      " 0.99466665 0.99525925 0.97511369 0.98874247 0.99140783 0.99140783\n",
      " 0.99466753 0.99466753 0.99466665 0.99525925 0.78458115 0.8112347\n",
      " 0.82042649 0.81242867 0.80975805 0.79731006 0.80295595 0.80117729\n",
      " 0.99111109 0.99703615 0.99822222 0.99851895 0.9982231  0.99881657\n",
      " 0.99881657 0.99852071 0.99111109 0.99703615 0.99822222 0.99851895\n",
      " 0.9982231  0.99881657 0.99881657 0.99852071 0.86666637 0.9102172\n",
      " 0.91378329 0.91822116 0.92029656 0.91940811 0.92148263 0.92651485\n",
      " 0.9946649  0.99940741 0.99970326 0.99970414 0.99970414 0.99970414\n",
      " 1.         0.99940828 0.9946649  0.99940741 0.99970326 0.99970414\n",
      " 0.99970414 0.99970414 1.         0.99940828 0.91348217 0.95022036\n",
      " 0.95970625 0.96385528 0.97127017 0.96948888 0.97215511 0.97659649\n",
      " 0.99762875 0.99940653 1.         1.         1.         1.\n",
      " 1.         1.         0.99762875 0.99940653 1.         1.\n",
      " 1.         1.         1.         1.         0.94163784 0.97422348\n",
      " 0.98044704 0.98755641 0.99022615 0.9920013  0.9934841  0.99496515\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.96504311 0.99081875\n",
      " 0.99437606 0.99674556 0.99703966 0.9973364  0.99881657 0.99911243\n",
      " 0.99970326 1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99970326 1.         1.         1.\n",
      " 1.         1.         1.         1.         0.97777992 0.99674381\n",
      " 0.99881657 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.98725879 0.99792812\n",
      " 0.99940741 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.98963005 0.99851895\n",
      " 0.99940741 1.         1.         1.         1.         1.\n",
      " 0.94459379 0.96207399 0.96562692 0.96533896 0.96859252 0.9700762\n",
      " 0.97007445 0.96889014 0.94459379 0.96207399 0.96562692 0.96533896\n",
      " 0.96859252 0.9700762  0.97007445 0.96889014 0.6755544  0.6737775\n",
      " 0.66756361 0.65274086 0.64503801 0.63407195 0.6243025  0.61807543\n",
      " 0.97332976 0.98636771 0.98755289 0.98755377 0.98933331 0.99081523\n",
      " 0.99110846 0.99318473 0.97332976 0.98636771 0.98755289 0.98755377\n",
      " 0.98933331 0.99081523 0.99110846 0.99318473 0.76976718 0.7827981\n",
      " 0.79525574 0.78487788 0.77954103 0.76502028 0.76888136 0.76058943\n",
      " 0.98667059 0.9931821  0.99644356 0.99644356 0.99762875 0.99762962\n",
      " 0.99851983 0.99881657 0.98667059 0.9931821  0.99644356 0.99644356\n",
      " 0.99762875 0.99762962 0.99851983 0.99881657 0.8539357  0.88473478\n",
      " 0.89037715 0.89215055 0.89629343 0.89006198 0.89333222 0.89184679\n",
      " 0.99348147 0.99733201 0.99822134 0.99911155 0.99911155 0.99970414\n",
      " 0.99970414 0.99940828 0.99348147 0.99733201 0.99822134 0.99911155\n",
      " 0.99911155 0.99970414 0.99970414 0.99940828 0.89809141 0.93747652\n",
      " 0.94459379 0.94696504 0.94933278 0.95377768 0.95792671 0.95644479\n",
      " 0.99525749 0.99911067 0.99970326 1.         0.99970414 1.\n",
      " 1.         1.         0.99525749 0.99911067 0.99970326 1.\n",
      " 0.99970414 1.         1.         1.         0.92415764 0.9617755\n",
      " 0.97126227 0.976298   0.98192808 0.98281829 0.98578038 0.98726318\n",
      " 0.99792636 1.         0.99970414 0.99970414 0.99970414 1.\n",
      " 1.         1.         0.99792636 1.         0.99970414 0.99970414\n",
      " 0.99970414 1.         1.         1.         0.95615859 0.98518866\n",
      " 0.98904096 0.99111373 0.99377908 0.99467017 0.99674205 0.99674293\n",
      " 0.99940741 1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99940741 1.         1.         1.\n",
      " 1.         1.         1.         1.         0.97214984 0.99170544\n",
      " 0.99555686 0.99763138 0.99851895 0.99822397 0.99940828 0.99911243\n",
      " 0.99970414 1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99970414 1.         1.         1.\n",
      " 1.         1.         1.         1.         0.98251804 0.99703703\n",
      " 0.99792724 0.99940828 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.98755816 0.99822134\n",
      " 0.99970414 0.99970414 1.         1.         1.         1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7708392603129445\n",
      "testing classifiers: ['GaussianNB']\n",
      "running: GaussianNB\n",
      "0.6331436699857753\n",
      "testing classifiers: ['KNeighborsClassifier']\n",
      "running: KNeighborsClassifier\n",
      "0.5999288762446657\n",
      "testing classifiers: ['AdaBoostClassifier']\n",
      "running: AdaBoostClassifier\n",
      "0.9123044096728309\n"
     ]
    }
   ],
   "source": [
    "burchard_zwickau_greed_rearch_resp = []\n",
    "for cls in ['SVC', 'DecisionTreeClassifier', 'GaussianProcessClassifier', 'RandomForestClassifier', 'GaussianNB', 'KNeighborsClassifier', 'AdaBoostClassifier']:\n",
    "    grid_search_cv_result = thesisModelFeatures.run_grid_search_cv(burchard_zwickau_features_tfidf_2_5_gram_cosine_similarity_long_p_df, [cls])\n",
    "    burchard_zwickau_greed_rearch_resp.append([cls, grid_search_cv_result[1][0].best_score_])\n",
    "    print(grid_search_cv_result[1][0].best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "319ee97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['SVC', 0.768421052631579],\n",
       " ['DecisionTreeClassifier', 0.7571123755334283],\n",
       " ['GaussianProcessClassifier', 0.7524182076813657],\n",
       " ['RandomForestClassifier', 0.7708392603129445],\n",
       " ['GaussianNB', 0.6331436699857753],\n",
       " ['KNeighborsClassifier', 0.5999288762446657],\n",
       " ['AdaBoostClassifier', 0.9123044096728309]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burchard_zwickau_greed_rearch_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bcfc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random zwickau VS random zwickau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c07005",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indexes = np.split(\n",
    "    np.random.choice(\n",
    "        range(\n",
    "            0, \n",
    "            len(zwickau_poorly_similar_with_chops_corpus_without_word_processing_long_p)\n",
    "        ), \n",
    "        size=len(zwickau_poorly_similar_with_chops_corpus_without_word_processing_long_p), \n",
    "        replace=False\n",
    "    ),\n",
    "    2\n",
    ")\n",
    "random_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8517c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zwickau_random_chunk_1 = [ zwickau_poorly_similar_with_chops_corpus_without_word_processing_long_p[i] for i in random_indexes[0] ]\n",
    "zwickau_random_chunk_2 = [ zwickau_poorly_similar_with_chops_corpus_without_word_processing_long_p[i] for i in random_indexes[1] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zwickau_zwickau_features_tfidf_2_5_gram_cosine_similarity_long_p_df = thesisModelFeatures.create_features_df(\n",
    "    None,\n",
    "    zwickau_random_chunk_1,\n",
    "    zwickau_random_chunk_2,\n",
    "    n_gram = (2,5),\n",
    "    features = { 'tfidf', 'inner_mean_cosine_similarity_score' }\n",
    "#     burchard_version_with_original_london_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb27c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zwickau_zwickau_tf_idf_2_5_cosine_results_long_p = thesisModelFeatures.run_models(zwickau_zwickau_features_tfidf_2_5_gram_cosine_similarity_long_p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zwickau_zwickau_tf_idf_2_5_cosine_results_long_p.sort_values(by=['accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f73442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3068dd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>corpus_version_label</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a a</th>\n",
       "      <th>a ad</th>\n",
       "      <th>a b</th>\n",
       "      <th>a be</th>\n",
       "      <th>a c</th>\n",
       "      <th>a ca</th>\n",
       "      <th>...</th>\n",
       "      <th>zuri</th>\n",
       "      <th>zurio</th>\n",
       "      <th>zy</th>\n",
       "      <th>zyd</th>\n",
       "      <th>zyda</th>\n",
       "      <th>zyda</th>\n",
       "      <th>zyp</th>\n",
       "      <th>zyph</th>\n",
       "      <th>zyph</th>\n",
       "      <th>inner_mean_cosine_similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>151.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065443</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.236613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046068</td>\n",
       "      <td>0.023691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066390</td>\n",
       "      <td>0.045345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016036</td>\n",
       "      <td>0.022043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>155.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>156.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062593</td>\n",
       "      <td>0.037310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.023451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.195983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.022415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.014891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.026525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.015494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.050490</td>\n",
       "      <td>0.027588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.035416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.017490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.091024</td>\n",
       "      <td>0.020942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.195953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.027158</td>\n",
       "      <td>0.039571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034985</td>\n",
       "      <td>0.048092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.070678</td>\n",
       "      <td>0.030895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.019351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.011916</td>\n",
       "      <td>0.026044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.039572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.038858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.019031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.086235</td>\n",
       "      <td>0.034269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.177432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.027307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.184727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.094245</td>\n",
       "      <td>0.008956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.036287</td>\n",
       "      <td>0.019828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.008424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.162578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.032775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.035740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.041482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.070284</td>\n",
       "      <td>0.034137</td>\n",
       "      <td>0.046697</td>\n",
       "      <td>0.046697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.028840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.018763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.161761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.059305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.038691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.038396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>31.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.050458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>32.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.039067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090968</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>34.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.036813</td>\n",
       "      <td>0.013410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.207159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.027177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.021974</td>\n",
       "      <td>0.036020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.179575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.060324</td>\n",
       "      <td>0.016481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.236860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>38.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.062484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.186265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>39.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.072698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.171306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.037541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.078212</td>\n",
       "      <td>0.031080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.036629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.202291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 51080 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  corpus_version_label         a        a        a a      a ad  \\\n",
       "150  150.0                   0.0  0.008741  0.000000  0.000000  0.000000   \n",
       "151  151.0                   0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "152  152.0                   0.0  0.065443  0.006653  0.000000  0.000000   \n",
       "153  153.0                   0.0  0.046068  0.023691  0.000000  0.000000   \n",
       "154  154.0                   0.0  0.066390  0.045345  0.000000  0.000000   \n",
       "155  155.0                   0.0  0.070168  0.000000  0.000000  0.000000   \n",
       "156  156.0                   0.0  0.062593  0.037310  0.000000  0.000000   \n",
       "157    0.0                   2.0  0.023451  0.000000  0.000000  0.000000   \n",
       "158    1.0                   2.0  0.022415  0.000000  0.000000  0.000000   \n",
       "159    2.0                   2.0  0.014891  0.000000  0.000000  0.000000   \n",
       "160    3.0                   2.0  0.026525  0.000000  0.000000  0.000000   \n",
       "161    4.0                   2.0  0.015494  0.000000  0.000000  0.000000   \n",
       "162    5.0                   2.0  0.050490  0.027588  0.000000  0.000000   \n",
       "163    6.0                   2.0  0.035416  0.000000  0.000000  0.000000   \n",
       "164    7.0                   2.0  0.017490  0.000000  0.000000  0.000000   \n",
       "165    8.0                   2.0  0.091024  0.020942  0.000000  0.000000   \n",
       "166    9.0                   2.0  0.027158  0.039571  0.000000  0.000000   \n",
       "167   10.0                   2.0  0.070678  0.030895  0.000000  0.000000   \n",
       "168   11.0                   2.0  0.019351  0.000000  0.000000  0.000000   \n",
       "169   12.0                   2.0  0.011916  0.026044  0.000000  0.000000   \n",
       "170   13.0                   2.0  0.039572  0.000000  0.000000  0.000000   \n",
       "171   14.0                   2.0  0.038858  0.000000  0.000000  0.000000   \n",
       "172   15.0                   2.0  0.019031  0.000000  0.000000  0.000000   \n",
       "173   16.0                   2.0  0.086235  0.034269  0.000000  0.000000   \n",
       "174   17.0                   2.0  0.027307  0.000000  0.000000  0.000000   \n",
       "175   18.0                   2.0  0.094245  0.008956  0.000000  0.000000   \n",
       "176   19.0                   2.0  0.036287  0.019828  0.000000  0.000000   \n",
       "177   20.0                   2.0  0.008424  0.000000  0.000000  0.000000   \n",
       "178   21.0                   2.0  0.027522  0.000000  0.000000  0.000000   \n",
       "179   22.0                   2.0  0.032775  0.000000  0.000000  0.000000   \n",
       "180   23.0                   2.0  0.035740  0.000000  0.000000  0.000000   \n",
       "181   24.0                   2.0  0.041482  0.000000  0.000000  0.000000   \n",
       "182   25.0                   2.0  0.070284  0.034137  0.046697  0.046697   \n",
       "183   26.0                   2.0  0.028840  0.000000  0.000000  0.000000   \n",
       "184   27.0                   2.0  0.018763  0.000000  0.000000  0.000000   \n",
       "185   28.0                   2.0  0.059305  0.000000  0.000000  0.000000   \n",
       "186   29.0                   2.0  0.038691  0.000000  0.000000  0.000000   \n",
       "187   30.0                   2.0  0.038396  0.000000  0.000000  0.000000   \n",
       "188   31.0                   2.0  0.050458  0.000000  0.000000  0.000000   \n",
       "189   32.0                   2.0  0.039067  0.000000  0.000000  0.000000   \n",
       "190   33.0                   2.0  0.090968  0.012426  0.000000  0.000000   \n",
       "191   34.0                   2.0  0.036813  0.013410  0.000000  0.000000   \n",
       "192   35.0                   2.0  0.027177  0.000000  0.000000  0.000000   \n",
       "193   36.0                   2.0  0.021974  0.036020  0.000000  0.000000   \n",
       "194   37.0                   2.0  0.060324  0.016481  0.000000  0.000000   \n",
       "195   38.0                   2.0  0.062484  0.000000  0.000000  0.000000   \n",
       "196   39.0                   2.0  0.072698  0.000000  0.000000  0.000000   \n",
       "197   40.0                   2.0  0.037541  0.000000  0.000000  0.000000   \n",
       "198   41.0                   2.0  0.078212  0.031080  0.000000  0.000000   \n",
       "199   42.0                   2.0  0.036629  0.000000  0.000000  0.000000   \n",
       "\n",
       "          a b      a be       a c      a ca  ...  zuri  zurio   zy  zyd  zyda  \\\n",
       "150  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "151  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "152  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "153  0.013338  0.013338  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "154  0.000000  0.000000  0.016036  0.022043  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "155  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "156  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "157  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "158  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "159  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "160  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "161  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "162  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "163  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "164  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "165  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "166  0.000000  0.000000  0.034985  0.048092  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "167  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "168  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "169  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "170  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "171  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "172  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "173  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "174  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "175  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "176  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "177  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "178  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "179  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "180  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "181  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "182  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "183  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "184  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "185  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "186  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "187  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "188  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "189  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "190  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "191  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "192  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "193  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "194  0.000000  0.000000  0.029141  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "195  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "196  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "197  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "198  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "199  0.000000  0.000000  0.000000  0.000000  ...   0.0    0.0  0.0  0.0   0.0   \n",
       "\n",
       "     zyda   zyp  zyph  zyph   inner_mean_cosine_similarity_score  \n",
       "150    0.0  0.0   0.0    0.0                            0.066978  \n",
       "151    0.0  0.0   0.0    0.0                            0.085383  \n",
       "152    0.0  0.0   0.0    0.0                            0.236613  \n",
       "153    0.0  0.0   0.0    0.0                            0.185026  \n",
       "154    0.0  0.0   0.0    0.0                            0.176503  \n",
       "155    0.0  0.0   0.0    0.0                            0.143079  \n",
       "156    0.0  0.0   0.0    0.0                            0.176597  \n",
       "157    0.0  0.0   0.0    0.0                            0.195983  \n",
       "158    0.0  0.0   0.0    0.0                            0.146909  \n",
       "159    0.0  0.0   0.0    0.0                            0.174106  \n",
       "160    0.0  0.0   0.0    0.0                            0.168113  \n",
       "161    0.0  0.0   0.0    0.0                            0.227799  \n",
       "162    0.0  0.0   0.0    0.0                            0.175704  \n",
       "163    0.0  0.0   0.0    0.0                            0.182638  \n",
       "164    0.0  0.0   0.0    0.0                            0.159420  \n",
       "165    0.0  0.0   0.0    0.0                            0.195953  \n",
       "166    0.0  0.0   0.0    0.0                            0.133535  \n",
       "167    0.0  0.0   0.0    0.0                            0.174380  \n",
       "168    0.0  0.0   0.0    0.0                            0.141743  \n",
       "169    0.0  0.0   0.0    0.0                            0.107023  \n",
       "170    0.0  0.0   0.0    0.0                            0.115572  \n",
       "171    0.0  0.0   0.0    0.0                            0.168396  \n",
       "172    0.0  0.0   0.0    0.0                            0.130073  \n",
       "173    0.0  0.0   0.0    0.0                            0.177432  \n",
       "174    0.0  0.0   0.0    0.0                            0.184727  \n",
       "175    0.0  0.0   0.0    0.0                            0.226649  \n",
       "176    0.0  0.0   0.0    0.0                            0.214811  \n",
       "177    0.0  0.0   0.0    0.0                            0.162578  \n",
       "178    0.0  0.0   0.0    0.0                            0.204071  \n",
       "179    0.0  0.0   0.0    0.0                            0.150124  \n",
       "180    0.0  0.0   0.0    0.0                            0.136210  \n",
       "181    0.0  0.0   0.0    0.0                            0.196587  \n",
       "182    0.0  0.0   0.0    0.0                            0.159988  \n",
       "183    0.0  0.0   0.0    0.0                            0.157107  \n",
       "184    0.0  0.0   0.0    0.0                            0.161761  \n",
       "185    0.0  0.0   0.0    0.0                            0.205984  \n",
       "186    0.0  0.0   0.0    0.0                            0.178163  \n",
       "187    0.0  0.0   0.0    0.0                            0.165199  \n",
       "188    0.0  0.0   0.0    0.0                            0.107526  \n",
       "189    0.0  0.0   0.0    0.0                            0.209388  \n",
       "190    0.0  0.0   0.0    0.0                            0.152665  \n",
       "191    0.0  0.0   0.0    0.0                            0.207159  \n",
       "192    0.0  0.0   0.0    0.0                            0.122932  \n",
       "193    0.0  0.0   0.0    0.0                            0.179575  \n",
       "194    0.0  0.0   0.0    0.0                            0.236860  \n",
       "195    0.0  0.0   0.0    0.0                            0.186265  \n",
       "196    0.0  0.0   0.0    0.0                            0.171306  \n",
       "197    0.0  0.0   0.0    0.0                            0.172283  \n",
       "198    0.0  0.0   0.0    0.0                            0.235712  \n",
       "199    0.0  0.0   0.0    0.0                            0.202291  \n",
       "\n",
       "[50 rows x 51080 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burchard_london_features_tfidf_2_5_gram_cosine_similarity_long_p_df[150:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a712cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 5 has been classified as  2.0 and should be  0.0\n",
      "Row 7 has been classified as  2.0 and should be  0.0\n",
      "Row 9 has been classified as  2.0 and should be  0.0\n",
      "Row 15 has been classified as  2.0 and should be  0.0\n",
      "Row 170 has been classified as  0.0 and should be  2.0\n",
      "Row 172 has been classified as  0.0 and should be  2.0\n",
      "score is: 0.8378378378378378\n",
      "Row 20 has been classified as  2.0 and should be  0.0\n",
      "Row 23 has been classified as  2.0 and should be  0.0\n",
      "Row 179 has been classified as  0.0 and should be  2.0\n",
      "Row 182 has been classified as  0.0 and should be  2.0\n",
      "Row 188 has been classified as  0.0 and should be  2.0\n",
      "Row 190 has been classified as  0.0 and should be  2.0\n",
      "Row 192 has been classified as  0.0 and should be  2.0\n",
      "score is: 0.8108108108108109\n",
      "Row 33 has been classified as  2.0 and should be  0.0\n",
      "Row 35 has been classified as  2.0 and should be  0.0\n",
      "Row 36 has been classified as  2.0 and should be  0.0\n",
      "Row 40 has been classified as  2.0 and should be  0.0\n",
      "Row 41 has been classified as  2.0 and should be  0.0\n",
      "Row 42 has been classified as  2.0 and should be  0.0\n",
      "Row 43 has been classified as  2.0 and should be  0.0\n",
      "Row 44 has been classified as  2.0 and should be  0.0\n",
      "Row 202 has been classified as  0.0 and should be  2.0\n",
      "Row 203 has been classified as  0.0 and should be  2.0\n",
      "Row 204 has been classified as  0.0 and should be  2.0\n",
      "Row 216 has been classified as  0.0 and should be  2.0\n",
      "score is: 0.6756756756756757\n",
      "Row 54 has been classified as  2.0 and should be  0.0\n",
      "Row 55 has been classified as  2.0 and should be  0.0\n",
      "Row 57 has been classified as  2.0 and should be  0.0\n",
      "Row 226 has been classified as  0.0 and should be  2.0\n",
      "Row 227 has been classified as  0.0 and should be  2.0\n",
      "Row 236 has been classified as  0.0 and should be  2.0\n",
      "Row 240 has been classified as  0.0 and should be  2.0\n",
      "score is: 0.8108108108108109\n",
      "Row 68 has been classified as  2.0 and should be  0.0\n",
      "Row 69 has been classified as  2.0 and should be  0.0\n",
      "Row 73 has been classified as  2.0 and should be  0.0\n",
      "Row 74 has been classified as  2.0 and should be  0.0\n",
      "Row 76 has been classified as  2.0 and should be  0.0\n",
      "Row 241 has been classified as  0.0 and should be  2.0\n",
      "Row 245 has been classified as  0.0 and should be  2.0\n",
      "Row 247 has been classified as  0.0 and should be  2.0\n",
      "score is: 0.7837837837837838\n",
      "Row 81 has been classified as  2.0 and should be  0.0\n",
      "Row 82 has been classified as  2.0 and should be  0.0\n",
      "Row 84 has been classified as  2.0 and should be  0.0\n",
      "Row 85 has been classified as  2.0 and should be  0.0\n",
      "Row 86 has been classified as  2.0 and should be  0.0\n",
      "Row 88 has been classified as  2.0 and should be  0.0\n",
      "Row 89 has been classified as  2.0 and should be  0.0\n",
      "Row 91 has been classified as  2.0 and should be  0.0\n",
      "Row 92 has been classified as  2.0 and should be  0.0\n",
      "Row 93 has been classified as  2.0 and should be  0.0\n",
      "Row 263 has been classified as  0.0 and should be  2.0\n",
      "Row 270 has been classified as  0.0 and should be  2.0\n",
      "Row 272 has been classified as  0.0 and should be  2.0\n",
      "score is: 0.6486486486486487\n",
      "Row 97 has been classified as  2.0 and should be  0.0\n",
      "Row 107 has been classified as  2.0 and should be  0.0\n",
      "Row 298 has been classified as  0.0 and should be  2.0\n",
      "Row 301 has been classified as  0.0 and should be  2.0\n",
      "score is: 0.8888888888888888\n",
      "Row 113 has been classified as  2.0 and should be  0.0\n",
      "Row 114 has been classified as  2.0 and should be  0.0\n",
      "Row 116 has been classified as  2.0 and should be  0.0\n",
      "Row 119 has been classified as  2.0 and should be  0.0\n",
      "Row 121 has been classified as  2.0 and should be  0.0\n",
      "Row 122 has been classified as  2.0 and should be  0.0\n",
      "Row 123 has been classified as  2.0 and should be  0.0\n",
      "Row 124 has been classified as  2.0 and should be  0.0\n",
      "Row 307 has been classified as  0.0 and should be  2.0\n",
      "Row 309 has been classified as  0.0 and should be  2.0\n",
      "Row 311 has been classified as  0.0 and should be  2.0\n",
      "score is: 0.6944444444444444\n",
      "Row 127 has been classified as  2.0 and should be  0.0\n",
      "Row 129 has been classified as  2.0 and should be  0.0\n",
      "Row 131 has been classified as  2.0 and should be  0.0\n",
      "Row 133 has been classified as  2.0 and should be  0.0\n",
      "Row 137 has been classified as  2.0 and should be  0.0\n",
      "Row 141 has been classified as  2.0 and should be  0.0\n",
      "Row 332 has been classified as  0.0 and should be  2.0\n",
      "Row 340 has been classified as  0.0 and should be  2.0\n",
      "score is: 0.7777777777777778\n",
      "Row 142 has been classified as  2.0 and should be  0.0\n",
      "Row 145 has been classified as  2.0 and should be  0.0\n",
      "Row 147 has been classified as  2.0 and should be  0.0\n",
      "Row 149 has been classified as  2.0 and should be  0.0\n",
      "Row 152 has been classified as  2.0 and should be  0.0\n",
      "Row 153 has been classified as  2.0 and should be  0.0\n",
      "Row 154 has been classified as  2.0 and should be  0.0\n",
      "Row 155 has been classified as  2.0 and should be  0.0\n",
      "Row 156 has been classified as  2.0 and should be  0.0\n",
      "Row 349 has been classified as  0.0 and should be  2.0\n",
      "Row 362 has been classified as  0.0 and should be  2.0\n",
      "score is: 0.6944444444444444\n"
     ]
    }
   ],
   "source": [
    "wrong_redictions = thesisModelFeatures.get_model_wrong_prediction(burchard_london_features_tfidf_2_5_gram_cosine_similarity_long_p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e6aeec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../computed_data/models/burchard_london_features_tfidf_2_5_gram_cosine_similarity_long_p_wrong_predictions.npy', wrong_redictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "912cb813",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_redictions_2 = np.load('../computed_data/models/burchard_london_features_tfidf_2_5_gram_cosine_similarity_long_p_wrong_predictions.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "933028a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.0, 0.0, 5), (2.0, 0.0, 7), (2.0, 0.0, 9), (2.0, 0.0, 15), (0.0, 2.0, 170), (0.0, 2.0, 172)]\n",
      "[(2.0, 0.0, 20), (2.0, 0.0, 23), (0.0, 2.0, 179), (0.0, 2.0, 182), (0.0, 2.0, 188), (0.0, 2.0, 190), (0.0, 2.0, 192)]\n",
      "[(2.0, 0.0, 33), (2.0, 0.0, 35), (2.0, 0.0, 36), (2.0, 0.0, 40), (2.0, 0.0, 41), (2.0, 0.0, 42), (2.0, 0.0, 43), (2.0, 0.0, 44), (0.0, 2.0, 202), (0.0, 2.0, 203), (0.0, 2.0, 204), (0.0, 2.0, 216)]\n",
      "[(2.0, 0.0, 54), (2.0, 0.0, 55), (2.0, 0.0, 57), (0.0, 2.0, 226), (0.0, 2.0, 227), (0.0, 2.0, 236), (0.0, 2.0, 240)]\n",
      "[(2.0, 0.0, 68), (2.0, 0.0, 69), (2.0, 0.0, 73), (2.0, 0.0, 74), (2.0, 0.0, 76), (0.0, 2.0, 241), (0.0, 2.0, 245), (0.0, 2.0, 247)]\n",
      "[(2.0, 0.0, 81), (2.0, 0.0, 82), (2.0, 0.0, 84), (2.0, 0.0, 85), (2.0, 0.0, 86), (2.0, 0.0, 88), (2.0, 0.0, 89), (2.0, 0.0, 91), (2.0, 0.0, 92), (2.0, 0.0, 93), (0.0, 2.0, 263), (0.0, 2.0, 270), (0.0, 2.0, 272)]\n",
      "[(2.0, 0.0, 97), (2.0, 0.0, 107), (0.0, 2.0, 298), (0.0, 2.0, 301)]\n",
      "[(2.0, 0.0, 113), (2.0, 0.0, 114), (2.0, 0.0, 116), (2.0, 0.0, 119), (2.0, 0.0, 121), (2.0, 0.0, 122), (2.0, 0.0, 123), (2.0, 0.0, 124), (0.0, 2.0, 307), (0.0, 2.0, 309), (0.0, 2.0, 311)]\n",
      "[(2.0, 0.0, 127), (2.0, 0.0, 129), (2.0, 0.0, 131), (2.0, 0.0, 133), (2.0, 0.0, 137), (2.0, 0.0, 141), (0.0, 2.0, 332), (0.0, 2.0, 340)]\n",
      "[(2.0, 0.0, 142), (2.0, 0.0, 145), (2.0, 0.0, 147), (2.0, 0.0, 149), (2.0, 0.0, 152), (2.0, 0.0, 153), (2.0, 0.0, 154), (2.0, 0.0, 155), (2.0, 0.0, 156), (0.0, 2.0, 349), (0.0, 2.0, 362)]\n"
     ]
    }
   ],
   "source": [
    "for i in wrong_redictions_2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac257ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2.0, 0.0, 5),\n",
       "  (2.0, 0.0, 7),\n",
       "  (2.0, 0.0, 9),\n",
       "  (2.0, 0.0, 15),\n",
       "  (0.0, 2.0, 170),\n",
       "  (0.0, 2.0, 172)],\n",
       " [(2.0, 0.0, 20),\n",
       "  (2.0, 0.0, 23),\n",
       "  (0.0, 2.0, 179),\n",
       "  (0.0, 2.0, 182),\n",
       "  (0.0, 2.0, 188),\n",
       "  (0.0, 2.0, 190),\n",
       "  (0.0, 2.0, 192)],\n",
       " [(2.0, 0.0, 33),\n",
       "  (2.0, 0.0, 35),\n",
       "  (2.0, 0.0, 36),\n",
       "  (2.0, 0.0, 40),\n",
       "  (2.0, 0.0, 41),\n",
       "  (2.0, 0.0, 42),\n",
       "  (2.0, 0.0, 43),\n",
       "  (2.0, 0.0, 44),\n",
       "  (0.0, 2.0, 202),\n",
       "  (0.0, 2.0, 203),\n",
       "  (0.0, 2.0, 204),\n",
       "  (0.0, 2.0, 216)],\n",
       " [(2.0, 0.0, 54),\n",
       "  (2.0, 0.0, 55),\n",
       "  (2.0, 0.0, 57),\n",
       "  (0.0, 2.0, 226),\n",
       "  (0.0, 2.0, 227),\n",
       "  (0.0, 2.0, 236),\n",
       "  (0.0, 2.0, 240)],\n",
       " [(2.0, 0.0, 68),\n",
       "  (2.0, 0.0, 69),\n",
       "  (2.0, 0.0, 73),\n",
       "  (2.0, 0.0, 74),\n",
       "  (2.0, 0.0, 76),\n",
       "  (0.0, 2.0, 241),\n",
       "  (0.0, 2.0, 245),\n",
       "  (0.0, 2.0, 247)],\n",
       " [(2.0, 0.0, 81),\n",
       "  (2.0, 0.0, 82),\n",
       "  (2.0, 0.0, 84),\n",
       "  (2.0, 0.0, 85),\n",
       "  (2.0, 0.0, 86),\n",
       "  (2.0, 0.0, 88),\n",
       "  (2.0, 0.0, 89),\n",
       "  (2.0, 0.0, 91),\n",
       "  (2.0, 0.0, 92),\n",
       "  (2.0, 0.0, 93),\n",
       "  (0.0, 2.0, 263),\n",
       "  (0.0, 2.0, 270),\n",
       "  (0.0, 2.0, 272)],\n",
       " [(2.0, 0.0, 97), (2.0, 0.0, 107), (0.0, 2.0, 298), (0.0, 2.0, 301)],\n",
       " [(2.0, 0.0, 113),\n",
       "  (2.0, 0.0, 114),\n",
       "  (2.0, 0.0, 116),\n",
       "  (2.0, 0.0, 119),\n",
       "  (2.0, 0.0, 121),\n",
       "  (2.0, 0.0, 122),\n",
       "  (2.0, 0.0, 123),\n",
       "  (2.0, 0.0, 124),\n",
       "  (0.0, 2.0, 307),\n",
       "  (0.0, 2.0, 309),\n",
       "  (0.0, 2.0, 311)],\n",
       " [(2.0, 0.0, 127),\n",
       "  (2.0, 0.0, 129),\n",
       "  (2.0, 0.0, 131),\n",
       "  (2.0, 0.0, 133),\n",
       "  (2.0, 0.0, 137),\n",
       "  (2.0, 0.0, 141),\n",
       "  (0.0, 2.0, 332),\n",
       "  (0.0, 2.0, 340)],\n",
       " [(2.0, 0.0, 142),\n",
       "  (2.0, 0.0, 145),\n",
       "  (2.0, 0.0, 147),\n",
       "  (2.0, 0.0, 149),\n",
       "  (2.0, 0.0, 152),\n",
       "  (2.0, 0.0, 153),\n",
       "  (2.0, 0.0, 154),\n",
       "  (2.0, 0.0, 155),\n",
       "  (2.0, 0.0, 156),\n",
       "  (0.0, 2.0, 349),\n",
       "  (0.0, 2.0, 362)]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PAY ATTENTION: there 2 types of errors\n",
    "# burcahrd prediction and london prediction, it means that prediction for burchard is smaller\n",
    "wrong_redictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0339c883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "12\n",
      "7\n",
      "8\n",
      "13\n",
      "4\n",
      "11\n",
      "8\n",
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for i in wrong_redictions:\n",
    "    s += len(i)\n",
    "    print(len(i))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7f94a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(burchard_candidate_version_based_london_without_word_processing_long_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set([ result[2] for iteration in wrong_redictions for result in iteration ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f060141c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(london_poorly_similar_with_chops_corpus_without_word_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56cbfee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = base64.b64encode(london_poorly_similar_with_chops_corpus_without_word_processing[1].encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb3f06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {encoded: london_poorly_similar_with_chops_corpus_without_word_processing[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad40555d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'dmV0ZXJpYnVzIGh5c3RvcmlpcyBsZWdhbXVzIGJlYXR1cyBsaWJyaXMgcG9zaXRpIG1lbnBoaXRpY29zIGFwb2xsb25pdXMgY2F1Y2FzdW0gc2NpdGhhcyBpbmRpYW0gYnJhZ21hbm9zIHlhcnRoYW0gdmFuZXJhYmFudHVyIGNoZXJ1YmluIGZ1dHVyaSBhcHV0IHZlbmVyYWJpbGlzIHF1b2NpZW5zIHRvY2llbnMgbGludGhlYW1pbmlidXM=': 'veteribus hystoriis legamus beatus libris positi menphiticos apollonius caucasum scithas indiam bragmanos yartham vanerabantur cherubin futuri aput venerabilis quociens tociens lintheaminibus'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdd14dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_p_to_index_map = { p: i for i, p in enumerate(london_poorly_similar_with_chops_corpus_without_word_processing) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac640b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found discrepancy 0:1\n",
      "found discrepancy 1:5\n",
      "found discrepancy 2:12\n",
      "found discrepancy 4:16\n",
      "found discrepancy 7:22\n",
      "found discrepancy 8:23\n",
      "found discrepancy 9:24\n",
      "found discrepancy 10:25\n",
      "found discrepancy 11:26\n",
      "found discrepancy 12:27\n",
      "found discrepancy 13:28\n",
      "found discrepancy 14:31\n",
      "found discrepancy 17:37\n",
      "found discrepancy 19:42\n",
      "found discrepancy 20:44\n",
      "found discrepancy 21:45\n",
      "found discrepancy 22:46\n",
      "found discrepancy 24:50\n",
      "found discrepancy 25:51\n",
      "found discrepancy 26:53\n",
      "found discrepancy 27:55\n",
      "found discrepancy 28:56\n",
      "found discrepancy 31:59\n",
      "found discrepancy 32:60\n",
      "found discrepancy 34:64\n",
      "found discrepancy 37:71\n",
      "found discrepancy 38:72\n",
      "found discrepancy 39:74\n",
      "found discrepancy 43:78\n",
      "found discrepancy 44:79\n",
      "found discrepancy 45:80\n",
      "found discrepancy 47:83\n",
      "found discrepancy 50:86\n",
      "found discrepancy 51:87\n",
      "found discrepancy 52:89\n",
      "found discrepancy 54:92\n",
      "found discrepancy 56:94\n",
      "found discrepancy 57:95\n",
      "found discrepancy 59:98\n",
      "found discrepancy 61:100\n",
      "found discrepancy 62:102\n",
      "found discrepancy 63:103\n",
      "found discrepancy 64:104\n",
      "found discrepancy 66:109\n",
      "found discrepancy 67:112\n",
      "found discrepancy 69:114\n",
      "found discrepancy 70:115\n",
      "found discrepancy 71:116\n",
      "found discrepancy 72:117\n",
      "found discrepancy 74:122\n",
      "found discrepancy 75:124\n",
      "found discrepancy 76:126\n",
      "found discrepancy 77:128\n",
      "found discrepancy 78:129\n",
      "found discrepancy 79:130\n",
      "found discrepancy 80:131\n",
      "found discrepancy 83:141\n",
      "found discrepancy 84:142\n",
      "found discrepancy 85:144\n",
      "found discrepancy 86:145\n",
      "found discrepancy 87:146\n",
      "found discrepancy 88:147\n",
      "found discrepancy 89:148\n",
      "found discrepancy 90:150\n",
      "found discrepancy 91:152\n",
      "found discrepancy 92:153\n",
      "found discrepancy 93:154\n",
      "found discrepancy 95:157\n",
      "found discrepancy 98:161\n",
      "found discrepancy 99:162\n",
      "found discrepancy 100:163\n",
      "found discrepancy 101:164\n",
      "found discrepancy 102:170\n",
      "found discrepancy 103:171\n",
      "found discrepancy 104:173\n",
      "found discrepancy 106:175\n",
      "found discrepancy 107:179\n",
      "found discrepancy 108:180\n",
      "found discrepancy 109:182\n",
      "found discrepancy 110:186\n",
      "found discrepancy 111:187\n",
      "found discrepancy 112:188\n",
      "found discrepancy 114:191\n",
      "found discrepancy 115:194\n",
      "found discrepancy 117:196\n",
      "found discrepancy 118:197\n",
      "found discrepancy 119:204\n",
      "found discrepancy 120:205\n",
      "found discrepancy 123:208\n",
      "found discrepancy 125:212\n",
      "found discrepancy 128:215\n",
      "found discrepancy 129:216\n",
      "found discrepancy 130:217\n",
      "found discrepancy 133:220\n",
      "found discrepancy 134:221\n",
      "found discrepancy 135:223\n",
      "found discrepancy 136:224\n",
      "found discrepancy 137:225\n",
      "found discrepancy 139:227\n",
      "found discrepancy 140:228\n",
      "found discrepancy 141:229\n",
      "found discrepancy 142:230\n",
      "found discrepancy 144:232\n",
      "found discrepancy 145:233\n",
      "found discrepancy 146:234\n",
      "found discrepancy 147:254\n",
      "found discrepancy 148:259\n",
      "found discrepancy 149:265\n",
      "found discrepancy 150:284\n",
      "found discrepancy 151:305\n",
      "found discrepancy 152:308\n",
      "found discrepancy 153:309\n",
      "found discrepancy 154:310\n",
      "found discrepancy 155:311\n",
      "found discrepancy 156:312\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(london_poorly_similar_with_chops_corpus_without_word_processing_long_p):\n",
    "    original_index = london_p_to_index_map[p]\n",
    "    if p != thesisDataReader.get_london_by_new_line_without_words_processing()[original_index]:\n",
    "        print(f'found discrepancy {i}:{original_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3d2d014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sicut ecclesiastica ecclesia sancti sepulchri muro conclusus cuius tytulum vidi tam sedes archiepiscopalis metropolis phenicie habens suffraganeos beritensem sydoniensem acconensem episcopos extenditur metropolis usque ad petram incisam ut dictum sive castrum peregrinorum decem diebus fui pluribus vicibus aliis consideravi eam quantum potui diligenter'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "london_poorly_similar_with_chops_corpus_without_word_processing_long_p[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81a02583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "london_p_to_index_map[london_poorly_similar_with_chops_corpus_without_word_processing_long_p[12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7a2befa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multe sunt in civitate ista reliquie sicut colligitur ex hystoria ecclesiastica de martiribus sub dyocletiano ibi passis quorum numerus est soli deo notus origenes ibidem in ecclesia sancti sepulchri requiescit in muro conclusus cuius tytulum ibidem vidi sunt ibi columpne marmoree et aliorum lapidum tam magne quod stupor est videre sedes archiepiscopalis est in civitate ista et est metropolis phenicie habens suffraganeos beritensem sydoniensem et acconensem episcopos et extenditur ista metropolis usque ad petram incisam ut dictum est sive castrum peregrinorum decem diebus fui in ea et pluribus vicibus aliis et consideravi eam quantum potui diligenter'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thesisDataReader.get_london_by_new_line_without_words_processing()[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "389e7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_strongly_similar_text_p_zero = pd.read_csv('../computed_data/p_aligment/by_new_line/strongly_similar/london_zwickau_breslau.csv').drop(['Unnamed: 0'], axis=1).loc[0, 'london text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79436460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ p: i for i, p in enumerate(thesisDataReader.get_london_corpus()) }[london_strongly_similar_text_p_zero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "725c359c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>london text</th>\n",
       "      <th>zwickau text</th>\n",
       "      <th>zwickau p#</th>\n",
       "      <th>zwickau score</th>\n",
       "      <th>breslau text</th>\n",
       "      <th>breslau p#</th>\n",
       "      <th>breslau score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cum in ueteribus historiis legamus sicut dicit...</td>\n",
       "      <td>cum sicut dicit ieronimus quosdam inueniamus l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.812947</td>\n",
       "      <td>uenerabantur quondam iudei sancta sanctorum qu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.228345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quis cristianus hiis uisis non festinet uenire...</td>\n",
       "      <td>quis cristianus hiis uisis non festinabit ueni...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>uenerabantur quondam iudei sancta sanctorum qu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.326126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>redeat post hec in ierusalem ut uideat et audi...</td>\n",
       "      <td>redeat post in ierusalem uideat et audiat in t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.887962</td>\n",
       "      <td>uenerabantur quondam iudei sancta sanctorum qu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.486159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sunt preterea quedam que in locis ilis deuotio...</td>\n",
       "      <td>sunt preterea quedam que in locis ilis deuotio...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.815394</td>\n",
       "      <td>sunt preterea que deuotionem excitant ampliore...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.358153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eia domine deus uideo secundum ueteres histori...</td>\n",
       "      <td>eia domine deus uideo secundum ueteres histori...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.864038</td>\n",
       "      <td>sunt preterea que deuotionem excitant ampliore...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.397394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>de lippari nauigantes per miliaria uenimus in ...</td>\n",
       "      <td>de lippari nauigantes per miliaria uenimus in ...</td>\n",
       "      <td>212</td>\n",
       "      <td>0.871923</td>\n",
       "      <td>sunt preterea que deuotionem excitant ampliore...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.120797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>de strongoli igitur mouentes auta et sperantes...</td>\n",
       "      <td>de strongoli igitur mouentes auta et sperantes...</td>\n",
       "      <td>213</td>\n",
       "      <td>0.917238</td>\n",
       "      <td>procedendo igitur de accon per primam diuisioe...</td>\n",
       "      <td>24</td>\n",
       "      <td>0.078676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>egressi itaque de galea uenimus boias de quo l...</td>\n",
       "      <td>egressi itaque de galea uenimus baias de quo l...</td>\n",
       "      <td>214</td>\n",
       "      <td>0.891483</td>\n",
       "      <td>regia erat hec ciuitas in monte pulcra nimis u...</td>\n",
       "      <td>40</td>\n",
       "      <td>0.134890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>in neapoli in castro saluatoris uidi ouum uirg...</td>\n",
       "      <td>in neapoli in castro saluatoris uidi ouum uirg...</td>\n",
       "      <td>215</td>\n",
       "      <td>0.857767</td>\n",
       "      <td>uenerabantur quondam iudei sancta sanctorum qu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.110843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>de roma per arecium et florentiam ueni bononia...</td>\n",
       "      <td>de roma per arecium et florentiam ueni bonnoni...</td>\n",
       "      <td>216</td>\n",
       "      <td>0.844623</td>\n",
       "      <td>sunt preterea que deuotionem excitant ampliore...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.063840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>218 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           london text  \\\n",
       "0    cum in ueteribus historiis legamus sicut dicit...   \n",
       "1    quis cristianus hiis uisis non festinet uenire...   \n",
       "2    redeat post hec in ierusalem ut uideat et audi...   \n",
       "3    sunt preterea quedam que in locis ilis deuotio...   \n",
       "4    eia domine deus uideo secundum ueteres histori...   \n",
       "..                                                 ...   \n",
       "213  de lippari nauigantes per miliaria uenimus in ...   \n",
       "214  de strongoli igitur mouentes auta et sperantes...   \n",
       "215  egressi itaque de galea uenimus boias de quo l...   \n",
       "216  in neapoli in castro saluatoris uidi ouum uirg...   \n",
       "217  de roma per arecium et florentiam ueni bononia...   \n",
       "\n",
       "                                          zwickau text  zwickau p#  \\\n",
       "0    cum sicut dicit ieronimus quosdam inueniamus l...           0   \n",
       "1    quis cristianus hiis uisis non festinabit ueni...           1   \n",
       "2    redeat post in ierusalem uideat et audiat in t...           2   \n",
       "3    sunt preterea quedam que in locis ilis deuotio...           3   \n",
       "4    eia domine deus uideo secundum ueteres histori...           4   \n",
       "..                                                 ...         ...   \n",
       "213  de lippari nauigantes per miliaria uenimus in ...         212   \n",
       "214  de strongoli igitur mouentes auta et sperantes...         213   \n",
       "215  egressi itaque de galea uenimus baias de quo l...         214   \n",
       "216  in neapoli in castro saluatoris uidi ouum uirg...         215   \n",
       "217  de roma per arecium et florentiam ueni bonnoni...         216   \n",
       "\n",
       "     zwickau score                                       breslau text  \\\n",
       "0         0.812947  uenerabantur quondam iudei sancta sanctorum qu...   \n",
       "1         0.896212  uenerabantur quondam iudei sancta sanctorum qu...   \n",
       "2         0.887962  uenerabantur quondam iudei sancta sanctorum qu...   \n",
       "3         0.815394  sunt preterea que deuotionem excitant ampliore...   \n",
       "4         0.864038  sunt preterea que deuotionem excitant ampliore...   \n",
       "..             ...                                                ...   \n",
       "213       0.871923  sunt preterea que deuotionem excitant ampliore...   \n",
       "214       0.917238  procedendo igitur de accon per primam diuisioe...   \n",
       "215       0.891483  regia erat hec ciuitas in monte pulcra nimis u...   \n",
       "216       0.857767  uenerabantur quondam iudei sancta sanctorum qu...   \n",
       "217       0.844623  sunt preterea que deuotionem excitant ampliore...   \n",
       "\n",
       "     breslau p#  breslau score  \n",
       "0             1       0.228345  \n",
       "1             1       0.326126  \n",
       "2             1       0.486159  \n",
       "3             2       0.358153  \n",
       "4             2       0.397394  \n",
       "..          ...            ...  \n",
       "213           2       0.120797  \n",
       "214          24       0.078676  \n",
       "215          40       0.134890  \n",
       "216           1       0.110843  \n",
       "217           2       0.063840  \n",
       "\n",
       "[218 rows x 7 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../computed_data/p_aligment/by_new_line/strongly_similar/london_zwickau_breslau.csv').drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b0a78120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.utils' from '../src/utils/utils.py'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(thesisDataReader)\n",
    "imp.reload(thesisUtils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e98d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = thesisDataReader.create_burchard_to_london_text_to_text_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e2606b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cum in sicut dicit ieronimus quosdam lustrasse provincias maria transfretasse ut ea que ex noverant coram viderent ut plato vates et egyptum qui persas intravit transivit albanos massagethas quoque ut videret et tandem egyptum intravit ut famosam mensam solis videret in sabulo quid mirum si christiani terram illam quam christi sonant ecclesie universe videre et visitare desiderant antiqui sancta sanctorum quia ibi erat archa testamenti et cum propiciatorio et manna et virga aaron que fronduerat que omnia erant umbra nonne nos est sepulchrum dulcis ihesu quod quis ingreditur involutum syndone mentis videt oculis salvatorem et paululum procedens videt lapidem revolutum angelum in eo sedentem et sudarium cum mulieribus ostendentem',\n",
       " 'cum in veteribus hystoriis legamus sicut dicit beatus ieronimus quosdam lustrasse provincias maria transfretasse ut ea que ex libris noverant coram positi viderent ut plato menphiticos vates et egyptum apollonius qui persas intravit transivit caucasum albanos scithas massagethas indiam bragmanos quoque ut yartham videret et tandem egyptum intravit ut famosam mensam solis videret in sabulo quid mirum si christiani terram illam quam christi sonant ecclesie universe videre et visitare desiderant vanerabantur antiqui sancta sanctorum quia ibi erat archa testamenti et cherubin cum propiciatorio et manna et virga aaron que fronduerat que omnia erant umbra futuri nonne aput nos venerabilis est sepulchrum dulcis ihesu quod quociens quis ingreditur tociens involutum syndone mentis videt oculis salvatorem et paululum procedens videt lapidem revolutum angelum in eo sedentem et sudarium cum lintheaminibus mulieribus ostendentem')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cf019e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>burchard</th>\n",
       "      <th>london</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cum in sicut dicit ieronimus quosdam lustrasse...</td>\n",
       "      <td>cum in veteribus hystoriis legamus sicut dicit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quis christianus hiis visis non venire in beth...</td>\n",
       "      <td>quis christianus hiis visis non festinet venir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>redeat post hec in ierusalem ut videat et audi...</td>\n",
       "      <td>redeat post hec in ierusalem ut videat et audi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sunt preterea quedam que in locis illis devoci...</td>\n",
       "      <td>sunt preterea quedam que in locis illis devoci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eia domine deus video secundum veteres hystori...</td>\n",
       "      <td>eia domine deus video secundum veteres hystori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>de lippari navigantes per miliaria venimus in ...</td>\n",
       "      <td>de lippari navigantes per miliaria venimus in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>de strongoli igitur moventes vela et sperantes...</td>\n",
       "      <td>de strongoli igitur moventes vela et sperantes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>egressi itaque de galea venimus de quo loco di...</td>\n",
       "      <td>egressi itaque de galea venimus boyas de quo l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>in neapoli in castro salvatoris vidi ovum virg...</td>\n",
       "      <td>in neapoli in castro salvatoris vidi ovum virg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>de roma per arecium et florentiam veni ad magi...</td>\n",
       "      <td>de roma per arecium et florentiam veni bononia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>218 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              burchard  \\\n",
       "0    cum in sicut dicit ieronimus quosdam lustrasse...   \n",
       "1    quis christianus hiis visis non venire in beth...   \n",
       "2    redeat post hec in ierusalem ut videat et audi...   \n",
       "3    sunt preterea quedam que in locis illis devoci...   \n",
       "4    eia domine deus video secundum veteres hystori...   \n",
       "..                                                 ...   \n",
       "213  de lippari navigantes per miliaria venimus in ...   \n",
       "214  de strongoli igitur moventes vela et sperantes...   \n",
       "215  egressi itaque de galea venimus de quo loco di...   \n",
       "216  in neapoli in castro salvatoris vidi ovum virg...   \n",
       "217  de roma per arecium et florentiam veni ad magi...   \n",
       "\n",
       "                                                london  \n",
       "0    cum in veteribus hystoriis legamus sicut dicit...  \n",
       "1    quis christianus hiis visis non festinet venir...  \n",
       "2    redeat post hec in ierusalem ut videat et audi...  \n",
       "3    sunt preterea quedam que in locis illis devoci...  \n",
       "4    eia domine deus video secundum veteres hystori...  \n",
       "..                                                 ...  \n",
       "213  de lippari navigantes per miliaria venimus in ...  \n",
       "214  de strongoli igitur moventes vela et sperantes...  \n",
       "215  egressi itaque de galea venimus boyas de quo l...  \n",
       "216  in neapoli in castro salvatoris vidi ovum virg...  \n",
       "217  de roma per arecium et florentiam veni bononia...  \n",
       "\n",
       "[218 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res, columns=['burchard', 'london'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36465c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
